{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the codes and experiment on your own!\n",
    "\n",
    "---\n",
    "\n",
    "The problem covered in this notebook is quite a common one in Data Science. Suppose you work at a Bank as a Data Scientist or Analyst, and your task is to predict the **Credit Rating** of a customer (for a specific credit account) given the duration and amount of the credit, payment history of previous credits, as well as the employment status, gender and age of the customer. Check out the data in the attached file `CreditDefault.csv` to get a clearer picture.    \n",
    "\n",
    "**Yes, you are absolutely right!** This is indeed a standard *Binary Classification* problem. Let's try out a few fun things on this dataset!\n",
    "\n",
    "---\n",
    "\n",
    "### Essential Libraries\n",
    "\n",
    "Let us begin by importing the essential Python Libraries.\n",
    "\n",
    "> NumPy : Library for Numeric Computations in Python  \n",
    "> Pandas : Library for Data Acquisition and Preparation  \n",
    "> Matplotlib : Low-level library for Data Visualization  \n",
    "> Seaborn : Higher-level library for Data Visualization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt # we only need pyplot\n",
    "sb.set() # set the default Seaborn style for graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup : Import the Dataset\n",
    "\n",
    "Dataset on Credit Rating : Attached file `CreditDefault.csv`     \n",
    "You know by now what to do with any dataset after you import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditData = pd.read_csv('CreditDefault.csv')\n",
    "creditData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list down all our observations from the super-quick glance of the dataset, as above.\n",
    "* There are `7` variables/features/columns and `1000` observations/samples/rows in the dataset.    \n",
    "* The response variable seems to be `Rating`, while the remaining 6 are most likely predictors.     \n",
    "* There are `3` variables identified as `int64` by default, and it seems they are indeed Numeric.     \n",
    "* There are `4` variables identified as `object` by default, and they are most likely Categorical.      \n",
    "* None of the variables/features seem to have any missing value (have to check again, carefully)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "Let us start by exploring the response variable `Rating`, the one we are supposed to predict at the end of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.catplot(y = 'Rating', data = creditData, kind = \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countG, countB = creditData['Rating'].value_counts()\n",
    "print(\"Ratio of classes is Good : Bad = \", countG, \":\", countB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright. We see a large class imbalance in the Response variable.     \n",
    "Unless we fix this, Classification models may face some problems.    \n",
    "\n",
    "Anyway, let's go ahead as it is, for the time being. May be later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly check out the Numeric and Categorical predictors separately, as the standard functions behave differently for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Predictors\n",
    "creditData[['Duration','Amount','Age']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Predictors\n",
    "creditData[['History','EmpStatus','Gender']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Not much yet. We just know that the variables and their values/ranges kind of make sense.   \n",
    "* Duration seems to be in Months, as the mean is around 20, while the min and max are 4 and 72.    \n",
    "* Amount seems to be in the currency amount (probably Dollars), guessing from the distribution.      \n",
    "* Age seems to be logical and in the correct scale, with min = 19 (legal age > 18?) and max = 75.    \n",
    "* History has 5 levels, with the customers paying previous credits duly seem to be the majority (530).     \n",
    "* Employment Status seems to be in number of years; 5 levels, with majority (339) less than 4 years.      \n",
    "* Gender has 2 levels, most likely Male and Female, with the majority (690) being Male customers.      \n",
    "\n",
    "If you need to know more about each variable, this is a good time to ask the person who actually collected the data or put together the dataset. You may also ask them about which variables they think are more important, as a part of \"domain knowledge\". However, we will work *blindly* on this dataset, as of now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for some visualizations to check the distribution of the predictor variables, one by one, for both Numeric and Categorical predictors. However, we will not just do it sequentially as they appear in the dataset as columms. We know better -- there are 2 predictors corresponding to the specific credit, while the other 4 predictors correspond to the specific customer. Let's check the predictors in that order to make more sense of our dataset, and gain better insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors corresponding to the Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of the Credit\n",
    "f, axes = plt.subplots(1, 2, figsize=(18, 4))\n",
    "sb.boxplot(data = creditData['Amount'], orient = \"h\", ax = axes[0])\n",
    "sb.histplot(data = creditData['Amount'], ax = axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What can you observe from the distribution of Amount?* \n",
    "- Try to write a few points based on the distribution above.\n",
    "- Try to talk to your friend(s) and see what the others think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration of the Credit\n",
    "f, axes = plt.subplots(1, 2, figsize=(18, 4))\n",
    "sb.boxplot(data = creditData['Duration'], orient = \"h\", ax = axes[0])\n",
    "sb.histplot(data = creditData['Duration'], ax = axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What can you observe from the distribution of Duration?* \n",
    "- Try to write a few points based on the distribution above.\n",
    "- Try to talk to your friend(s) and see what the others think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors corresponding to the Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age of the Customer\n",
    "f, axes = plt.subplots(1, 2, figsize=(18, 4))\n",
    "sb.boxplot(data = creditData['Duration'], orient = \"h\", ax = axes[0])\n",
    "sb.histplot(data = creditData['Duration'], ax = axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What can you observe from the distribution of Age?* \n",
    "- Try to write a few points based on the distribution above.\n",
    "- Try to talk to your friend(s) and see what the others think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender of the Customer\n",
    "sb.catplot(y = 'Gender', data = creditData, kind = \"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What can you observe from the distribution of Gender?* \n",
    "- Try to write a few points based on the distribution above.\n",
    "- Try to talk to your friend(s) and see what the others think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employment Status of the Customer\n",
    "sb.catplot(y = 'EmpStatus', data = creditData, kind = \"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What can you observe from the distribution of Employment Status?* \n",
    "- Try to write a few points based on the distribution above.\n",
    "- Try to talk to your friend(s) and see what the others think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# History of Payment\n",
    "sb.catplot(y = 'History', data = creditData, kind = \"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What can you observe from the distribution of History?* \n",
    "- Try to write a few points based on the distribution above.\n",
    "- Try to talk to your friend(s) and see what the others think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship with Rating\n",
    "\n",
    "Ok. Now that we understand the individual predictors better, let's find out what causes Good or Bad Rating. Is it the credit Amount or Duration in general, or does the Rating actually depend on the features related to the customer? Check mutual relationships to find out *potential* effect of predictors on Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating vs Amount\n",
    "f = plt.figure(figsize=(16, 8))\n",
    "sb.stripplot(x = 'Amount', y = 'Rating', data = creditData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Do you think credit Amount affects Rating directly?*\n",
    "- Try to write a few points based on the plots you tried out.\n",
    "- Try to talk to your friend(s) and see what the others think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating vs Duration\n",
    "f = plt.figure(figsize=(16, 8))\n",
    "sb.stripplot(x = 'Duration', y = 'Rating', data = creditData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Do you think credit Duration affects Rating directly?*\n",
    "- Try to write a few points based on the plots you tried out.\n",
    "- Try to talk to your friend(s) and see what the others think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating vs Age\n",
    "f = plt.figure(figsize=(16, 8))\n",
    "sb.stripplot(x = 'Age', y = 'Rating', data = creditData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Do you think credit Rating depends on the Age of customer?*\n",
    "- Try to write a few points based on the plots you tried out.\n",
    "- Try to talk to your friend(s) and see what the others think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating vs Gender\n",
    "f = plt.figure(figsize=(6, 4))\n",
    "sb.heatmap(creditData.groupby(['Rating', 'Gender']).size().unstack(), \n",
    "           linewidths = 1, annot = True, fmt = 'g', annot_kws = {\"size\": 18}, cmap = \"BuGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Do you think credit Rating depends on the Gender of customer?*\n",
    "- Try to write a few points based on the plots you tried out.\n",
    "- Try to talk to your friend(s) and see what the others think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating vs Employment Status\n",
    "f = plt.figure(figsize=(15, 4))\n",
    "sb.heatmap(creditData.groupby(['Rating', 'EmpStatus']).size().unstack(), \n",
    "           linewidths = 1, annot = True, fmt = 'g', annot_kws = {\"size\": 18}, cmap = \"BuGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Do you think credit Rating depends on the Employment Status of customer?*\n",
    "- Try to write a few points based on the plots you tried out.\n",
    "- Try to talk to your friend(s) and see what the others think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating vs History of Payment\n",
    "f = plt.figure(figsize=(15, 4))\n",
    "sb.heatmap(creditData.groupby(['Rating', 'History']).size().unstack(), \n",
    "           linewidths = 1, annot = True, fmt = 'g', annot_kws = {\"size\": 18}, cmap = \"BuGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Do you think credit Rating depends on the payment History of customer?*\n",
    "- Try to write a few points based on the plots you tried out.\n",
    "- Try to talk to your friend(s) and see what the others think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Relationships with Rating\n",
    "\n",
    "Let's try to explore slightly more complex bi-variate relationships of predictors with Rating, and see if we can observe something more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating vs Amount and Duration\n",
    "f = plt.figure(figsize=(16, 8))\n",
    "sb.scatterplot(x = 'Duration', y = 'Amount', hue = 'Rating', data = creditData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Anything more interesting based on the plot above?* \n",
    "- Write your comments based on the plot you tried out above.\n",
    "- Try out other similar Numeric-vs-Numeric plots with Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating vs Age and Gender\n",
    "sb.catplot(x = 'Age', y = 'Rating', row = 'Gender', data = creditData, kind = 'box', aspect = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Anything more interesting based on the plot above?* \n",
    "- Write your comments based on the plot you tried out above.\n",
    "- Try out other similar Numeric-vs-Categorical plots with Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are satisfied with basic EDA, proceed to building the actual Classification Model on the dataset, as follows. Note that after building the model, fitting it properly on the train data, and getting predictions out of it, you will again need to come back to EDA to connect the observations and comments you made initially with what you obtain from the model. Thus, EDA is not just a routine check; it is one of the most crucial stages of Data Science. Do pay attention!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating a Model for Rating : Attempt 1\n",
    "\n",
    "We will start with what we already know -- a multi-variate Classification Tree with all 6 predictors and Rating as response.     \n",
    "\n",
    "**Note** : `DecisionTreeClassifier` in `sklearn` does not handle categorical variables directly as a part of the current implementation. Thus, you will need to encode the levels of a categorical variable as integers. However, as the categorical variables may not be *ordinal*, you can't enforce an order with integer encoding (e.g., you can't encode Male and Female as 0 and 1 in Gender, as it enforces an implicit order). One way to encode nominal (unordered) categorical variables by integers is `OneHotEncoding`, and you can use it from `sklearn` preprocessing module. Do check out more about this encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the encoder from sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "# OneHotEncoding of categorical predictors (not the response)\n",
    "creditData_cat = creditData[['Gender','EmpStatus','History']]\n",
    "ohe.fit(creditData_cat)\n",
    "creditData_cat_ohe = pd.DataFrame(ohe.transform(creditData_cat).toarray(), \n",
    "                                  columns=ohe.get_feature_names_out(creditData_cat.columns))\n",
    "\n",
    "# Check the encoded variables\n",
    "creditData_cat_ohe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Numeric features with the OHE Categorical features\n",
    "creditData_num = creditData[['Amount','Duration','Age']]\n",
    "creditData_res = creditData['Rating']\n",
    "creditData_ohe = pd.concat([creditData_num, creditData_cat_ohe, creditData_res], \n",
    "                           sort = False, axis = 1).reindex(index=creditData_num.index)\n",
    "\n",
    "# Check the final dataframe\n",
    "creditData_ohe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit the Model\n",
    "\n",
    "Finally, after the encoding is done, we can create and fit the `DecisionTreeClassifier` model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential models and functions from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = pd.DataFrame(creditData_ohe['Rating'])\n",
    "X = pd.DataFrame(creditData_ohe.drop('Rating', axis = 1))\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 4)  # change max_depth to experiment\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Plot the trained Decision Tree\n",
    "f = plt.figure(figsize=(24,24))\n",
    "plot_tree(dectree, filled=True, rounded=True, \n",
    "          feature_names=X_train.columns, \n",
    "          class_names=[\"Bad\",\"Good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy of the Model\n",
    "\n",
    "Print the Classification Accuracy and all other Accuracy Measures from the Confusion Matrix.  \n",
    "\n",
    "| Confusion Matrix  |       |        |        |      \n",
    "| :---              | :---: | :----: | :----: |         \n",
    "| Actual Negative   |  (0)  |   TN   |   FP   |             \n",
    "| Actual Positive   |  (1)  |   FN   |   TP   |       \n",
    "|                   |       |   (0)   |   (1)   |       \n",
    "|                   |       | Predicted Negative    |   Predicted Postitive  |     \n",
    "\n",
    "\n",
    "* `TPR = TP / (TP + FN)` : True Positive Rate = True Positives / All Positives    \n",
    "* `TNR = TN / (TN + FP)` : True Negative Rate = True Negatives / All Negatives    \n",
    "\n",
    "* `FPR = FP / (TN + FP)` : False Positive Rate = False Positives / All Negatives \n",
    "* `FNR = FN / (TP + FN)` : False Negative Rate = False Negatives / All Positives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Response corresponding to Predictors\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Train Data\")\n",
    "print(\"Accuracy  :\\t\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTrain = confusion_matrix(y_train, y_train_pred)\n",
    "tpTrain = cmTrain[1][1] # True Positives : Good (1) predicted Good (1)\n",
    "fpTrain = cmTrain[0][1] # False Positives : Bad (0) predicted Good (1)\n",
    "tnTrain = cmTrain[0][0] # True Negatives : Bad (0) predicted Bad (0)\n",
    "fnTrain = cmTrain[1][0] # False Negatives : Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Train :\\t\", (tpTrain/(tpTrain + fnTrain)))\n",
    "print(\"TNR Train :\\t\", (tnTrain/(tnTrain + fpTrain)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Train :\\t\", (fpTrain/(tnTrain + fpTrain)))\n",
    "print(\"FNR Train :\\t\", (fnTrain/(tpTrain + fnTrain)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required metric from sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the Response corresponding to Predictors\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Test Data\")\n",
    "print(\"Accuracy  :\\t\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTest = confusion_matrix(y_test, y_test_pred)\n",
    "tpTest = cmTest[1][1] # True Positives : Good (1) predicted Good (1)\n",
    "fpTest = cmTest[0][1] # False Positives : Bad (0) predicted Good (1)\n",
    "tnTest = cmTest[0][0] # True Negatives : Bad (0) predicted Bad (0)\n",
    "fnTest = cmTest[1][0] # False Negatives : Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Test :\\t\", (tpTest/(tpTest + fnTest)))\n",
    "print(\"TNR Test :\\t\", (tnTest/(tnTest + fpTest)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Test :\\t\", (fpTest/(fpTest + tnTest)))\n",
    "print(\"FNR Test :\\t\", (fnTest/(fnTest + tpTest)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important** : Note the huge imbalance in the *False Positives* and *False Negatives* in the confusion matrix. *False Positives* are much higher in number than *False Negatives* in both Train and Test data. This is not surprising -- actually, this is a direct effect of the large `Good` vs `Bad` class imbalance in the response variable `Rating`. As `Rating = Good` was more likely in the data, *False Positives* are more likely too. Let's see if we can fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create a Model for Rating : Attempt 2\n",
    "\n",
    "Let's refine the previous model by balancing the classes of the response `Rating` in the training data. Keeping the OneHotEncoding same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample Bad to match Good\n",
    "from sklearn.utils import resample\n",
    "\n",
    "creditBad = creditData_ohe[creditData_ohe.Rating == 'Bad']\n",
    "creditGood = creditData_ohe[creditData_ohe.Rating == 'Good']\n",
    " \n",
    "# Upsample the Bad samples\n",
    "creditBad_up = resample(creditBad, \n",
    "                        replace=True,                     # sample with replacement\n",
    "                        n_samples=creditGood.shape[0])    # to match number of Good\n",
    " \n",
    "# Combine the two classes back after upsampling\n",
    "creditData_ohe_up = pd.concat([creditGood, creditBad_up])\n",
    " \n",
    "# Check the ratio of the classes\n",
    "creditData_ohe_up['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot to check the balanced classes visually\n",
    "sb.catplot(y = 'Rating', data = creditData_ohe_up, kind = \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the OHE is still in place\n",
    "# and that the samples have now increased\n",
    "creditData_ohe_up.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit the Model\n",
    "\n",
    "Finally, after the encoding and upsampling is done, we can create and fit the `DecisionTreeClassifier` model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential models and functions from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = pd.DataFrame(creditData_ohe_up['Rating'])\n",
    "X = pd.DataFrame(creditData_ohe_up.drop('Rating', axis = 1))\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 4)  # change max_depth to experiment\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Plot the trained Decision Tree\n",
    "f = plt.figure(figsize=(24,24))\n",
    "plot_tree(dectree, filled=True, rounded=True, \n",
    "          feature_names=X_train.columns, \n",
    "          class_names=[\"Bad\",\"Good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy of the Model\n",
    "\n",
    "Print the Classification Accuracy and all other Accuracy Measures from the Confusion Matrix.  \n",
    "\n",
    "| Confusion Matrix  |       |        |        |      \n",
    "| :---              | :---: | :----: | :----: |         \n",
    "| Actual Negative   |  (0)  |   TN   |   FP   |             \n",
    "| Actual Positive   |  (1)  |   FN   |   TP   |       \n",
    "|                   |       |   (0)   |   (1)   |       \n",
    "|                   |       | Predicted Negative    |   Predicted Postitive  |     \n",
    "\n",
    "\n",
    "* `TPR = TP / (TP + FN)` : True Positive Rate = True Positives / All Positives    \n",
    "* `TNR = TN / (TN + FP)` : True Negative Rate = True Negatives / All Negatives    \n",
    "\n",
    "* `FPR = FP / (TN + FP)` : False Positive Rate = False Positives / All Negatives \n",
    "* `FNR = FN / (TP + FN)` : False Negative Rate = False Negatives / All Positives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Response corresponding to Predictors\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Train Data\")\n",
    "print(\"Accuracy  :\\t\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTrain = confusion_matrix(y_train, y_train_pred)\n",
    "tpTrain = cmTrain[1][1] # True Positives : Good (1) predicted Good (1)\n",
    "fpTrain = cmTrain[0][1] # False Positives : Bad (0) predicted Good (1)\n",
    "tnTrain = cmTrain[0][0] # True Negatives : Bad (0) predicted Bad (0)\n",
    "fnTrain = cmTrain[1][0] # False Negatives : Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Train :\\t\", (tpTrain/(tpTrain + fnTrain)))\n",
    "print(\"TNR Train :\\t\", (tnTrain/(tnTrain + fpTrain)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Train :\\t\", (fpTrain/(tnTrain + fpTrain)))\n",
    "print(\"FNR Train :\\t\", (fnTrain/(tpTrain + fnTrain)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required metric from sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the Response corresponding to Predictors\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Test Data\")\n",
    "print(\"Accuracy  :\\t\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTest = confusion_matrix(y_test, y_test_pred)\n",
    "tpTest = cmTest[1][1] # True Positives : Good (1) predicted Good (1)\n",
    "fpTest = cmTest[0][1] # False Positives : Bad (0) predicted Good (1)\n",
    "tnTest = cmTest[0][0] # True Negatives : Bad (0) predicted Bad (0)\n",
    "fnTest = cmTest[1][0] # False Negatives : Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Test :\\t\", (tpTest/(tpTest + fnTest)))\n",
    "print(\"TNR Test :\\t\", (tnTest/(tnTest + fpTest)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Test :\\t\", (fpTest/(fpTest + tnTest)))\n",
    "print(\"FNR Test :\\t\", (fnTest/(fnTest + tpTest)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! The false positives and false negatives seem more balanced now. Good job with the upsampling. As always, there is room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create a Model for Rating : Attempt 3\n",
    "\n",
    "It is quite obvious that the Decision Tree is not doing too well, even after upsampling and encoding. It is time for us to move on to some other classifiers. Let's try another tree based classifier -- `RandomForest`, very close to Decision Tree in principle, but instead of a single tree, it uses an *ensemble* of trees for better classification. It is super-simple to find a new model in `sklearn`, read the documentation a little, and apply it immediately, as follows. Try it out!     \n",
    "\n",
    "We will keep all the goodness of upsampling and encoding from our previous attempts, and thus, we will use the final dataset we created in the last attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential models and functions from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = pd.DataFrame(creditData_ohe_up['Rating'])\n",
    "X = pd.DataFrame(creditData_ohe_up.drop('Rating', axis = 1))\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier model from Scikit-Learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the Random Forest object\n",
    "rforest = RandomForestClassifier(n_estimators = 100,  # n_estimators denote number of trees\n",
    "                                 max_depth = 4)       # set the maximum depth of each tree\n",
    "\n",
    "# Fit Random Forest on Train Data\n",
    "rforest.fit(X_train, y_train.Rating.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion_matrix from Scikit-Learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the Response corresponding to Predictors\n",
    "y_train_pred = rforest.predict(X_train)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Train Data\")\n",
    "print(\"Accuracy  :\\t\", rforest.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTrain = confusion_matrix(y_train, y_train_pred)\n",
    "tpTrain = cmTrain[1][1] # True Positives : Good (1) predicted Good (1)\n",
    "fpTrain = cmTrain[0][1] # False Positives : Bad (0) predicted Good (1)\n",
    "tnTrain = cmTrain[0][0] # True Negatives : Bad (0) predicted Bad (0)\n",
    "fnTrain = cmTrain[1][0] # False Negatives : Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Train :\\t\", (tpTrain/(tpTrain + fnTrain)))\n",
    "print(\"TNR Train :\\t\", (tnTrain/(tnTrain + fpTrain)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Train :\\t\", (fpTrain/(tnTrain + fpTrain)))\n",
    "print(\"FNR Train :\\t\", (fnTrain/(tpTrain + fnTrain)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required metric from sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the Response corresponding to Predictors\n",
    "y_test_pred = rforest.predict(X_test)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Test Data\")\n",
    "print(\"Accuracy  :\\t\", rforest.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTest = confusion_matrix(y_test, y_test_pred)\n",
    "tpTest = cmTest[1][1] # True Positives : Good (1) predicted Good (1)\n",
    "fpTest = cmTest[0][1] # False Positives : Bad (0) predicted Good (1)\n",
    "tnTest = cmTest[0][0] # True Negatives : Bad (0) predicted Bad (0)\n",
    "fnTest = cmTest[1][0] # False Negatives : Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Test :\\t\", (tpTest/(tpTest + fnTest)))\n",
    "print(\"TNR Test :\\t\", (tnTest/(tnTest + fpTest)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Test :\\t\", (fpTest/(fpTest + tnTest)))\n",
    "print(\"FNR Test :\\t\", (fnTest/(fnTest + tpTest)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it seems that we are not doing too well even with our new model family `RandomForest`. Check it out on Scikit-Learn to see how to improve upon the performance by tuning the paramters -- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html \n",
    "\n",
    "It turns out that the two major parameters (actually, called the *hyper-parameters* of this model) of Random Forest are `n_estimators` (the number of decision trees in the forest) and `max_depth` (of each decision tree in the forest). Let's play around a little with the parameters, and see if we can improve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase the Number of Decision Trees in the Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential models and functions from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = pd.DataFrame(creditData_ohe_up['Rating'])\n",
    "X = pd.DataFrame(creditData_ohe_up.drop('Rating', axis = 1))\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Import RandomForestClassifier model from Scikit-Learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the Random Forest object\n",
    "rforest = RandomForestClassifier(n_estimators = 1000,  # CHANGE AND EXPERIMENT\n",
    "                                 max_depth = 4)       # CHANGE AND EXPERIMENT\n",
    "\n",
    "# Fit Random Forest on Train Data\n",
    "rforest.fit(X_train, y_train.Rating.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion_matrix from Scikit-Learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the Response corresponding to Predictors\n",
    "y_train_pred = rforest.predict(X_train)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Train Data\")\n",
    "print(\"Accuracy  :\\t\", rforest.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTrain = confusion_matrix(y_train, y_train_pred)\n",
    "tpTrain = cmTrain[1][1] # True Positives : Good (1) predicted Good (1)\n",
    "fpTrain = cmTrain[0][1] # False Positives : Bad (0) predicted Good (1)\n",
    "tnTrain = cmTrain[0][0] # True Negatives : Bad (0) predicted Bad (0)\n",
    "fnTrain = cmTrain[1][0] # False Negatives : Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Train :\\t\", (tpTrain/(tpTrain + fnTrain)))\n",
    "print(\"TNR Train :\\t\", (tnTrain/(tnTrain + fpTrain)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Train :\\t\", (fpTrain/(tnTrain + fpTrain)))\n",
    "print(\"FNR Train :\\t\", (fnTrain/(tpTrain + fnTrain)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required metric from sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the Response corresponding to Predictors\n",
    "y_test_pred = rforest.predict(X_test)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Test Data\")\n",
    "print(\"Accuracy  :\\t\", rforest.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTest = confusion_matrix(y_test, y_test_pred)\n",
    "tpTest = cmTest[1][1] # True Positives : Good (1) predicted Good (1)\n",
    "fpTest = cmTest[0][1] # False Positives : Bad (0) predicted Good (1)\n",
    "tnTest = cmTest[0][0] # True Negatives : Bad (0) predicted Bad (0)\n",
    "fnTest = cmTest[1][0] # False Negatives : Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Test :\\t\", (tpTest/(tpTest + fnTest)))\n",
    "print(\"TNR Test :\\t\", (tnTest/(tnTest + fpTest)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Test :\\t\", (fpTest/(fpTest + tnTest)))\n",
    "print(\"FNR Test :\\t\", (fnTest/(fnTest + tpTest)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase the Depth of Decision Trees in the Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential models and functions from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = pd.DataFrame(creditData_ohe_up['Rating'])\n",
    "X = pd.DataFrame(creditData_ohe_up.drop('Rating', axis = 1))\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Import RandomForestClassifier model from Scikit-Learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the Random Forest object\n",
    "rforest = RandomForestClassifier(n_estimators = 100,  # CHANGE AND EXPERIMENT\n",
    "                                 max_depth = 10)       # CHANGE AND EXPERIMENT\n",
    "\n",
    "# Fit Random Forest on Train Data\n",
    "rforest.fit(X_train, y_train.Rating.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion_matrix from Scikit-Learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the Response corresponding to Predictors\n",
    "y_train_pred = rforest.predict(X_train)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Train Data\")\n",
    "print(\"Accuracy  :\\t\", rforest.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTrain = confusion_matrix(y_train, y_train_pred)\n",
    "tpTrain = cmTrain[1][1] # True Positives : Good (1) predicted Good (1)\n",
    "fpTrain = cmTrain[0][1] # False Positives : Bad (0) predicted Good (1)\n",
    "tnTrain = cmTrain[0][0] # True Negatives : Bad (0) predicted Bad (0)\n",
    "fnTrain = cmTrain[1][0] # False Negatives : Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Train :\\t\", (tpTrain/(tpTrain + fnTrain)))\n",
    "print(\"TNR Train :\\t\", (tnTrain/(tnTrain + fpTrain)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Train :\\t\", (fpTrain/(tnTrain + fpTrain)))\n",
    "print(\"FNR Train :\\t\", (fnTrain/(tpTrain + fnTrain)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required metric from sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the Response corresponding to Predictors\n",
    "y_test_pred = rforest.predict(X_test)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Test Data\")\n",
    "print(\"Accuracy  :\\t\", rforest.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTest = confusion_matrix(y_test, y_test_pred)\n",
    "tpTest = cmTest[1][1] # True Positives : Good (1) predicted Good (1)\n",
    "fpTest = cmTest[0][1] # False Positives : Bad (0) predicted Good (1)\n",
    "tnTest = cmTest[0][0] # True Negatives : Bad (0) predicted Bad (0)\n",
    "fnTest = cmTest[1][0] # False Negatives : Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Test :\\t\", (tpTest/(tpTest + fnTest)))\n",
    "print(\"TNR Test :\\t\", (tnTest/(tnTest + fpTest)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Test :\\t\", (fpTest/(fpTest + tnTest)))\n",
    "print(\"FNR Test :\\t\", (fnTest/(fnTest + tpTest)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase both Number and Depth of Decision Trees in the Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential models and functions from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = pd.DataFrame(creditData_ohe_up['Rating'])\n",
    "X = pd.DataFrame(creditData_ohe_up.drop('Rating', axis = 1))\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Import RandomForestClassifier model from Scikit-Learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the Random Forest object\n",
    "rforest = RandomForestClassifier(n_estimators = 1000,  # CHANGE AND EXPERIMENT\n",
    "                                 max_depth = 10)       # CHANGE AND EXPERIMENT\n",
    "\n",
    "# Fit Random Forest on Train Data\n",
    "rforest.fit(X_train, y_train.Rating.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion_matrix from Scikit-Learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the Response corresponding to Predictors\n",
    "y_train_pred = rforest.predict(X_train)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Train Data\")\n",
    "print(\"Accuracy  :\\t\", rforest.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTrain = confusion_matrix(y_train, y_train_pred)\n",
    "tpTrain = cmTrain[1][1] # True Positives : Good (1) predicted Good (1)\n",
    "fpTrain = cmTrain[0][1] # False Positives : Bad (0) predicted Good (1)\n",
    "tnTrain = cmTrain[0][0] # True Negatives : Bad (0) predicted Bad (0)\n",
    "fnTrain = cmTrain[1][0] # False Negatives : Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Train :\\t\", (tpTrain/(tpTrain + fnTrain)))\n",
    "print(\"TNR Train :\\t\", (tnTrain/(tnTrain + fpTrain)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Train :\\t\", (fpTrain/(tnTrain + fpTrain)))\n",
    "print(\"FNR Train :\\t\", (fnTrain/(tpTrain + fnTrain)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required metric from sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the Response corresponding to Predictors\n",
    "y_test_pred = rforest.predict(X_test)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Test Data\")\n",
    "print(\"Accuracy  :\\t\", rforest.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTest = confusion_matrix(y_test, y_test_pred)\n",
    "tpTest = cmTest[1][1] # True Positives : Good (1) predicted Good (1)\n",
    "fpTest = cmTest[0][1] # False Positives : Bad (0) predicted Good (1)\n",
    "tnTest = cmTest[0][0] # True Negatives : Bad (0) predicted Bad (0)\n",
    "fnTest = cmTest[1][0] # False Negatives : Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Test :\\t\", (tpTest/(tpTest + fnTest)))\n",
    "print(\"TNR Test :\\t\", (tnTest/(tnTest + fpTest)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Test :\\t\", (fpTest/(fpTest + tnTest)))\n",
    "print(\"FNR Test :\\t\", (fnTest/(fnTest + tpTest)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with more number of trees and deeper trees in the forest, the accuracy of the Random Forest is getting better on both the train and test sets, but the gap between the performance on train and test is increasing too. This may be an indication of overfitting the train set. Read up more about over-fitting.      \n",
    "\n",
    "We will need to find the optimal hyper-parameters to maximize accuracy of the classifier on the test set in general. Also note that we are only trying out a single `train_test_split`, and it is impossible to predict the *generalized* performance of the model from a single run. Thus, we will need to run this multiple times, with randomized train and test sets, to get a better estimate. Both of these targets are achieved through Cross-Validation techniques, whereby you can optimize the hyper-parameters of a model. There are several tools available to perform Cross-Validation and Tune Hyper-parameters of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating a Model for Rating : Attempt 4\n",
    "\n",
    "Let's try out a basic set of tools for Cross-Validation, so that we can tune the Hyper-parameters of Random Forest in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier model from Scikit-Learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = pd.DataFrame(creditData_ohe_up['Rating'])\n",
    "X = pd.DataFrame(creditData_ohe_up.drop('Rating', axis = 1))\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearch for hyperparameter tuning using Cross-Validation (CV)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the Hyper-parameter Grid to search on, in case of Random Forest\n",
    "param_grid = {'n_estimators': np.arange(100,1001,100),   # number of trees 100, 200, ..., 1000\n",
    "              'max_depth': np.arange(2, 11)}             # depth of trees 2, 3, 4, 5, ..., 10\n",
    "\n",
    "# Create the Hyper-parameter Grid\n",
    "hpGrid = GridSearchCV(RandomForestClassifier(),   # the model family\n",
    "                      param_grid,                 # the search grid\n",
    "                      cv = 5,                     # 5-fold cross-validation\n",
    "                      scoring = 'accuracy')       # score to evaluate\n",
    "\n",
    "# Train the models using Cross-Validation\n",
    "hpGrid.fit(X_train, y_train.Rating.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the best Model or the best set of Hyper-parameters\n",
    "print(hpGrid.best_estimator_)\n",
    "\n",
    "# Print the score (accuracy) of the best Model after CV\n",
    "print(np.abs(hpGrid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Cross-Validation routine is selecting the `best_estimator_` based on the `scoring` parameter of `accuracy`. You may change it to something else, if you want. For instance, `f1` score will be a good `scoring` strategy for binary classification. You may also choose to score your models by their `roc_auc` (Area Under the Receiver Operating Characteristic Curve) metric for a more balanced approach. Read more about these metrics.\n",
    "\n",
    "F1 Score : https://en.wikipedia.org/wiki/F-score     \n",
    "ROC-AUC : https://en.wikipedia.org/wiki/Receiver_operating_characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Best Model found through GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential models and functions from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = pd.DataFrame(creditData_ohe_up['Rating'])\n",
    "X = pd.DataFrame(creditData_ohe_up.drop('Rating', axis = 1))\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Import RandomForestClassifier model from Scikit-Learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the Random Forest object\n",
    "rforest = RandomForestClassifier(n_estimators = 400,   # found using GridSearchCV\n",
    "                                 max_depth = 10)       # found using GridSearchCV\n",
    "\n",
    "# Fit Random Forest on Train Data\n",
    "rforest.fit(X_train, y_train.Rating.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion_matrix from Scikit-Learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the Response corresponding to Predictors\n",
    "y_train_pred = rforest.predict(X_train)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Train Data\")\n",
    "print(\"Accuracy  :\\t\", rforest.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTrain = confusion_matrix(y_train, y_train_pred)\n",
    "tpTrain = cmTrain[1][1] # True Positives : Good (1) predicted Good (1)\n",
    "fpTrain = cmTrain[0][1] # False Positives : Bad (0) predicted Good (1)\n",
    "tnTrain = cmTrain[0][0] # True Negatives : Bad (0) predicted Bad (0)\n",
    "fnTrain = cmTrain[1][0] # False Negatives : Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Train :\\t\", (tpTrain/(tpTrain + fnTrain)))\n",
    "print(\"TNR Train :\\t\", (tnTrain/(tnTrain + fpTrain)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Train :\\t\", (fpTrain/(tnTrain + fpTrain)))\n",
    "print(\"FNR Train :\\t\", (fnTrain/(tpTrain + fnTrain)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required metric from sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the Response corresponding to Predictors\n",
    "y_test_pred = rforest.predict(X_test)\n",
    "\n",
    "# Print the Classification Accuracy\n",
    "print(\"Test Data\")\n",
    "print(\"Accuracy  :\\t\", rforest.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Print the Accuracy Measures from the Confusion Matrix\n",
    "cmTest = confusion_matrix(y_test, y_test_pred)\n",
    "tpTest = cmTest[1][1] # True Positives : Good (1) predicted Good (1)\n",
    "fpTest = cmTest[0][1] # False Positives : Bad (0) predicted Good (1)\n",
    "tnTest = cmTest[0][0] # True Negatives : Bad (0) predicted Bad (0)\n",
    "fnTest = cmTest[1][0] # False Negatives : Good (1) predicted Bad (0)\n",
    "\n",
    "print(\"TPR Test :\\t\", (tpTest/(tpTest + fnTest)))\n",
    "print(\"TNR Test :\\t\", (tnTest/(tnTest + fpTest)))\n",
    "print()\n",
    "\n",
    "print(\"FPR Test :\\t\", (fpTest/(fpTest + tnTest)))\n",
    "print(\"FNR Test :\\t\", (fnTest/(fnTest + tpTest)))\n",
    "\n",
    "# Plot the two-way Confusion Matrix\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are done with our model building exercise. Is this complete? Not yet. We have not considered a few other issues that may have resulted in a bad model, like outliers and other imperfections in the dataset. We did not scale the predictors either, or consider other encodings for more ordinal categorical variables like `EmpStatus` and `History`. Thus, what we did today is just about alright as a preliminary model, and there is ample scope for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try exploring the following on your own, at your own time. These may help you in your project!**\n",
    "* Connecting back the predictions from your model to the EDA you did earlier, and your observations.\n",
    "* Trying other strategies for encoding of categorical variables to be used in the classification models.\n",
    "* Trying out other sampling strategies (or something else) for solving the class imabalance problem.\n",
    "* Trying out `GridSearchCV` with any other model of your choice (DecisionTreeClassifier, if you wish)\n",
    "* Using Random Forest family of models for Regression instead of Classification (yes, it is possible!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
